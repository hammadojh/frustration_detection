paper,predict_features,detect_features,download_status,download_link,paper_year,#detect_features,#predict_features,training_dataset(s),testing_dataset(s),architecture_summary,results_summary,limitations_summary,additional_features
Hernandez Caralt et al. (2024/2025). Stupid robot  I want to speak to a human!: User Frustration Detection in TOD. arXiv:2411.17437; COLING Industry 2025,,escalation_request,DOWNLOADED,https://aclanthology.org/2025.coling-industry.23.pdf,2025.0,1.0,0.0,Internal benchmark from a deployed TOD system,Internal benchmark from a deployed TOD system,"Compares a deployed keyword-based approach, open-source approaches to sentiment analysis, dialog breakdown detection methods, and in-context learning LLM-based detection for user frustration detection.","The LLM-based approach outperforms all other methods on the internal benchmark, achieving a 16% relative improvement in F1 score.","The study is based on an internal dataset, which is not publicly available. The authors also note that their analysis is limited to textual data and does not include other modalities like speech.",textual features
Fourier GNN MERC (2025 AAAI). https://ojs.aaai.org/index.php/AAAI/article/view/33242/35397,,emotion_volatility,DOWNLOADED,https://ojs.aaai.org/index.php/AAAI/article/view/33242/35397,2025.0,1.0,0.0,IEMOCAP and MELD,IEMOCAP and MELD,Proposes a Graph-Spectrum-based Multimodal Consistency and Complementary collaborative learning framework (GS-MCC) for Multimodal Emotion Recognition in Conversations (MERC). GS-MCC uses a sliding window to construct a multimodal interaction graph and designs efficient Fourier graph operators (FGO) to extract long-distance high-frequency and low-frequency information.,"The proposed GS-MCC architecture outperforms previous state-of-the-art models on two benchmark datasets, IEMOCAP and MELD.","The authors do not explicitly state limitations, but the work is focused on the specific task of multimodal emotion recognition in conversations.","multimodal features (text, audio, video)"
Zhang et al (2025),personality_traits,trust_in_system,DOWNLOADED,https://arxiv.org/html/2503.07279v1,2025.0,4.0,4.0,Multi-agent collaboration system,Human-agent conversations,Multi-agent LLM system with Mixtral-8x7B for trust evaluation,Real-time trust assessment across competence integrity benevolence predictability dimensions,Limited to text-based analysis lacks multimodal input capabilities,text-based features
Counterfactual Assessment of USE (2024),trust_in_system,,DOWNLOADED,https://aclanthology.org/2024.findings-acl.871.pdf,2024.0,1.0,0.0,MultiWOZ and SGD,Augmented versions of MultiWOZ and SGD,Leverages large language models (LLMs) to generate counterfactual dialogues to augment existing task-oriented dialogue datasets. It then evaluates the performance of two open-source LLMs (Zephyr-7b-beta and Mistral-7B-Instruct) as few-shot user satisfaction estimators against fine-tuned models.,"Open-source LLMs, when used as few-shot user satisfaction estimators, show higher robustness to the increase in the number of dissatisfaction labels in the test collection than the fine-tuned state-of-the-art models.",The study focuses on turn-level satisfaction estimation and leaves dialogue-level estimation for future work. The research is exclusively on datasets in English.,
Hsu et al (2024),session_dropout,,DOWNLOADED,https://people.cs.nycu.edu.tw/~armuro/pubs/hsu-et-al-2024-ijhci.pdf,2024.0,0.0,1.0,Proprietary dataset from a global e-commerce platform,Proprietary dataset from a global e-commerce platform,"Proposes a Behavior-aware Hierarchical Transformer (BHT) model. BHT has three components: a behavior encoder to represent customer actions, a hierarchical Transformer to capture both short-term and long-term dependencies in user behavior, and a dropout predictor.","BHT significantly outperforms baseline models (including other Transformer-based models) in predicting session dropout at various early stages of the session. It achieves higher AUC and F1 scores, demonstrating the effectiveness of its hierarchical structure and behavior-aware components.","The study uses a proprietary dataset, limiting reproducibility. The authors suggest future work could explore more sophisticated temporal modeling, incorporate more side information (like product details or user demographics), and test the model's generalizability to other e-commerce platforms.","customer actions, side information (product details, user demographics)"
Li et al (2024),,alignment_failures,DOWNLOADED,https://aclanthology.org/2024.naacl-industry.25.pdf,2024.0,1.0,0.0,Phone call conversations from a healthcare conversational AI agent.,Phone call conversations from a healthcare conversational AI agent.,"Introduces MultConDB, a multimodal model for detecting dialogue breakdown in real-time. The model uses both audio and text signals, leveraging Wav2Vec2 for acoustic features and RoBERTa for textual features. It employs a cross-modal transformer to integrate information from both modalities.","Multimodal models outperform text-only models in detecting dialogue breakdowns. MultConDB achieves the best F1 score, indicating that a multimodal contextual model architecture is critical for this task.","The study uses a proprietary dataset from the healthcare domain, which is not publicly available. The model is designed for phone call conversations and may not be directly applicable to other communication channels.","audio, text signals (Wav2Vec2, RoBERTa)"
Peng et al (2024),personality_traits,,DOWNLOADED,https://www.nature.com/articles/s41598-024-53989-y,2024.0,1.0,1.0,Task-oriented (378 participants) and open-domain (186 participants) human-machine dialogues,10-fold cross-validation on same datasets,BERT-based binary classification models predicting 25 personality traits from user utterances,Open-domain dialogue achieved better accuracy than task-oriented; Extraversion predicted equally well (64%) from both dialogue types,60% accuracy limitation; models don't generalize across dialogue types; hand-crafted features limit system portability,user utterances
Balaraman et al (2023),,"alignment_failures, confusion_lexical_markers",DOWNLOADED,https://aclanthology.org/2023.sigdial-1.52.pdf,2023.0,2.0,0.0,REPAIR-QA,REPAIR-QA,Introduces the REPAIR-QA dataset for Third Position Repair (TPR) in conversational QA. Uses T5 and GPT-3 models to generate corrected question rewrites based on dialogue history.,"The fine-tuned T5 model (T5-REPAIR-QA) outperforms GPT-3 models in handling TPR, indicating that large language models struggle with this phenomenon out-of-the-box.","The collected TPRs are always in the third turn, an artifact of the data collection method. The data was collected via a dialogue completion task, which is less ecologically valid than a fully interactive setting.",dialogue history
Begovic et al (2023),,negation_frequency,DOWNLOADED,https://www.mdpi.com/2076-3417/13/13/7760,2023.0,1.0,0.0,"A corpus of 11,461 Bosnian-language tweets",A filtered subset of 417 tweets,"Proposes a sentiment analysis model for Bosnian-language tweets incorporating negation cues and 'AnA-words'. It uses various machine learning techniques, with the best performance from a CNN combined with a pre-trained Bosnian sentiment lexicon (BOSentiment).",The combined BOSentiment and CNN model achieved an accuracy of over 92% in classifying the sentiment of Bosnian tweets.,The study is specific to the Bosnian language and the Twitter platform. The authors suggest that the approach could be extended to other languages and platforms.,"negation cues, AnA-words"
"Siro, Aliannejadi & de Rijke (2023)",task_complexity,,DOWNLOADED,https://dl.acm.org/doi/10.1145/3624989,2023.0,1.0,1.0,Extended ReDial dataset (200 dialogues),5-fold cross-validation,Classical ML models for turn and dialogue-level prediction,Random Forest achieves ρ=0.7337 for turn-level and F1=0.80 for dialogue classification,Limited to traditional ML approaches and single domain,interest arousal
Survey: Emotion Survey: Emotion Recognition in Conversations (MDPI 2023). https://www.mdpi.com/2079-9292/12/22/4714,,emotion_volatility,DOWNLOADED,https://www.mdpi.com/2079-9292/12/22/4714,2023.0,6.0,7.0,Multiple datasets MELD IEMOCAP DailyDialog EmoryNLP,Same datasets with train/dev/test splits,Graph Neural Networks and Transformers for multimodal fusion,Distributed representations outperform feature-based; context and speaker dependencies crucial,Multimodal conflicts challenging; limited cross-domain generalization,"multimodal features (text, audio, video)"
Azaria et al (2023),response_latency,,DOWNLOADED,https://link.springer.com/article/10.1007/s12599-022-00755-x,2022.0,0.0,1.0,N/A (Lab Experiment),N/A (Lab Experiment),A between-subjects lab experiment (N=202) was conducted where participants interacted with a chatbot that responded either instantly or with a delay. The chatbot was developed using the Microsoft Bot Framework.,"A delayed response time has opposing effects on social presence and usage intentions depending on the user's prior experience with chatbots. For novice users, a delayed response has a positive effect on social presence, while for experienced users, the effect is negative.","The study operationalized prior experience as a dichotomous variable (novice vs. experienced), which doesn't capture the full spectrum of experience. The use of a student sample might limit generalizability. The study did not capture participants' initial expectations about chatbots. The experiment used a relatively short interaction time.",prior experience
Siro et al (2022),task_complexity,trust_in_system,DOWNLOADED,https://arxiv.org/abs/2204.12195,2022.0,1.0,1.0,ReDial dataset (40 dialogues),5-fold cross-validation,Multiple ML models for satisfaction prediction,Interest arousal achieves highest correlation (ρ=0.7903) with satisfaction,Small dataset and external assessors instead of real users,interest arousal
Wambugu et al (2022),,expressed_urgency,DOWNLOADED,https://aclanthology.org/2022.aacl-srw.10.pdf,2022.0,1.0,1.0,Mobile WACh NEO studies (11129 labeled messages),Train/test/validation split (70%/20%/10%),mBERT fine-tuned and attention-based deep learning with handcrafted features,mBERT 90% AUROC; attention-based 95.7% AUROC for in-the-moment detection,Performance drops significantly for early prediction; requires multilingual annotation effort,handcrafted features
Zhang et al (2022),task_complexity,,DOWNLOADED,https://dl.acm.org/doi/10.1145/3477495.3531798,2022.0,1.0,1.0,ReDial dataset (40 dialogues),5-fold cross-validation,Multiple ML models for satisfaction prediction,Interest arousal achieves highest correlation (ρ=0.7903) with satisfaction,Small dataset and external assessors instead of real users,interest arousal
Zuters & Leonova (2022),"emotion_volatility, emotion_words, sentiment_trajectory",,DOWNLOADED,https://www.bjmc.lu.lv/fileadmin/user_upload/lu_portal/projekti/bjmc/Contents/10_4_08_Leonova.pdf,2022.0,7.0,1.0,"Latvian dataset (283 dialogs, 688 turns) and English dataset (400 dialogs, 843 turns) from Twitter customer support",Leave-one-out cross-validation on same datasets,Neural network (multi-layered perceptrons) with bag-of-words and non-lexical means of expression features,Achieved 48.2% accuracy for English and 49.1% for Latvian with 7pp improvement using NLME features,Limited to European languages with similar cultural context; requires compatible annotated datasets,"bag-of-words, non-lexical means of expression features"
Leonova & Zuters (2021),,"emphasis_capitalization, exclamation_density",DOWNLOADED,https://aclanthology.org/2021.ranlp-1.93.pdf,2021.0,2.0,0.0,Latvian and English customer support tweets,Latvian and English customer support tweets,"Presents a neural network model to annotate frustration intensity in tweets. The model uses a bag-of-words representation with subword segmentation and non-lexical features like punctuation, capitalization, and emojis.",Adding non-lexical features to the model significantly improves performance in predicting frustration intensity. Subword segmentation also has a positive effect on accuracy.,"The study is based on Twitter data, which may not be generalizable to other platforms. The list of non-lexical features is not exhaustive and other features could be explored.","non-lexical features (punctuation, capitalization, emojis)"
Mehri & Eskenazi (2021),goal_completion_status,,DOWNLOADED,https://aclanthology.org/2021.sigdial-1.42.pdf,2021.0,0.0,1.0,Simulated dialogues in the Cambridge Restaurants domain.,Simulated dialogues in the Cambridge Restaurants domain.,Proposes a multi-objective reinforcement learning (MORL) setup for dialogue policy learning that blends task success (TS) and user satisfaction (US) as reward components. It uses a linear reward scalarization to combine the two components.,"Blending task success and user satisfaction as reward components increases user satisfaction without sacrificing task success, even in more hostile environments. The proposed analysis method provides insights into the learned dialogue behavior.","The study is based on simulated dialogues, and the findings may not be directly generalizable to real-world user interactions. The proposed weight balance for the reward components may not be optimal for all domains and systems.","task success, user satisfaction"
Sun et al (2021),typing_speed_change,,DOWNLOADED,https://arxiv.org/pdf/2105.03748,2021.0,2.0,1.0,USS dataset 6800 dialogues from multiple domains,USS dataset cross-validation,Hierarchical GRU for satisfaction prediction with action prediction,HiGRU best in-domain; BERT better cross-domain generalization,Limited to predefined tasks; requires human satisfaction annotation,user actions
WARSE (2021),,"emphasis_capitalization, exclamation_density",DOWNLOADED,https://www.warse.org/IJATCSE/static/pdf/file/ijatcse1371022021.pdf,2021.0,2.0,0.0,Twitter dataset from Kaggle,Same dataset,VADER sentiment analysis tool with punctuation analysis,Exclamation marks maximize positive/negative polarity and minimize neutral polarity,Limited to punctuation effects on sentiment; no predictive modeling for user behavior,punctuation
Yang & Hsu (2021),session_dropout,,DOWNLOADED,https://people.cs.nycu.edu.tw/~armuro/pubs/yang-et-al-2021-cui-ea.pdf,2021.0,1.0,1.0,Banking chatbot logs (1837 users 17266 sequences),Cross-validation on same dataset,BERT end-to-end and attention-based deep learning with 102 handcrafted features,BERT 90% AUROC; attention-based 95.7% AUROC and 83.2% F1-score,Performance degrades for early prediction; annotation effort required for explainable model,102 handcrafted features
Chatterjee et al (2020),sentiment_trajectory,,DOWNLOADED,https://assets.amazon.science/1e/b3/daf26fef4a6ca4da9949a4b14c27/speech-sentiment-and-customer-satisfaction-estimation-in-socialbot-conversations.pdf,2020.0,0.0,1.0,Alexa Prize Socialbot data,Alexa Prize Socialbot data,"Proposes a two-phased modeling architecture to predict Customer Satisfaction (CSAT). First, it generates sentiment embeddings for each utterance using LSTMs on acoustic and lexical features. Second, it uses these embeddings to estimate conversation-level CSAT using both static (v-SVR) and temporal (BiLSTM) regression models.","The proposed temporal regression model (BiLSTM) outperforms the static baseline (v-SVR) and human performance in estimating CSAT. The results show that sentiment scores, particularly valence and satisfaction, are correlated with CSAT.","The study notes the difficulty for humans to judge CSAT scores, which may be due to the variety of factors that influence user satisfaction. The authors suggest that future work could explore integrating other factors besides expressed sentiment to improve CSAT prediction.","acoustic, lexical features"
Choi et al (2020),"reply_time_gap, response_latency, time_on_task",,DOWNLOADED,https://arxiv.org/pdf/2006.01921,2020.0,0.0,3.0,Dialogue Breakdown Detection Challenge 3 (DBDC3) and a large dataset from the Alexa Prize competition.,Dialogue Breakdown Detection Challenge 3 (DBDC3) and a large dataset from the Alexa Prize competition.,"Proposes a conversational satisfaction prediction model called ConvSAT. The model aggregates multiple representations of the conversation, including conversation history, utterance and response content, and behavioral signals. It uses a combination of contextualized word and character encoders, along with a behavioral feature matrix.",ConvSAT significantly improves satisfaction prediction for both offline and online settings on both datasets compared to previous state-of-the-art approaches.,"The authors note that while their model is a significant step forward, there is still room for improvement in accurately predicting user satisfaction in open-domain conversational systems.","conversation history, utterance and response content, behavioral signals, contextualized word and character encoders, behavioral feature matrix"
Landesberger et al (2020),,expressed_urgency,DOWNLOADED,https://www.semdial.org/anthology/Z20-Landesberger_semdial_0027.pdf,2020.0,1.0,0.0,A corpus of spoken urgent and non-urgent utterances from the game 'What is it?'.,A corpus of spoken urgent and non-urgent utterances from the game 'What is it?'.,"Uses various machine learning classifiers (RFC, GBC, LRC, KNNC, MLPC, SVMC) to detect urgency in speech based on acoustic features. It also explores personalization by adapting the acoustic features to individual speakers.","The models can detect urgency from acoustic features alone. Personalizing the acoustic features for each speaker improves the recognition of urgency, especially for the SVMC and MLPC classifiers.","The study is based on a specific game-based corpus, and the findings may not be generalizable to other types of dialogue. The authors suggest that future work could consider other aspects of the spoken utterance and the context, such as deictic expressions and the task addressed.",acoustic features
Park et al (2020),,negative_feedback,DOWNLOADED,https://assets.amazon.science/f4/af/0363d1a942259019f69027a3495f/large-scale-hybrid-approach-for-predicting-user-satisfaction-with-conversational-agents.pdf,2020.0,1.0,0.0,"Large-scale data from Amazon Alexa, including user feedback and human annotations.","Large-scale data from Amazon Alexa, including user feedback and human annotations.",Proposes a hybrid approach for predicting user satisfaction that fuses explicit user feedback with predictions from two machine-learned models: one trained on user feedback data and the other on human annotation data. The approach uses a waterfall policy to prioritize user feedback.,"The hybrid approach significantly improves user satisfaction prediction in terms of precision, recall, F1-score, and PR-AUC compared to a conventional approach based solely on human annotation. It also achieves more consistent performance across different domains.","The study acknowledges that user feedback is biased toward positive experiences and has limited coverage. The hybrid approach relies on a simple waterfall policy for fusion, and more sophisticated machine learning methods could be explored.",explicit user feedback
Ramesh et al (2020),,response_clarity,DOWNLOADED,https://assets.amazon.science/26/c6/36b3686044c6a7b521efdcc31ab5/joint-turn-and-dialogue-level-user-satisfaction-estimation-on-multi-domain-conversations.pdf,2020.0,1.0,1.0,3129 dialogue sessions (20167 turns) from 28 Alexa domains across two dialogue systems,80/10/10 train/validation/test split,"BiLSTM joint model with Universal Sentence Encoder embeddings, adaptive multi-task loss weighting, and attention mechanism",Joint model achieved 27% absolute improvement (0.43→0.70) in correlation and 7% improvement over benchmark models,Model not calibrated for user preferences/biases; requires attention mechanism for proper turn weighting,Universal Sentence Encoder embeddings
Zuters & Leonova (2020),"emotion_drift, negation_frequency, sentiment_trajectory",,DOWNLOADED,https://aircconline.com/ijcsit/V12N6/12620ijcsit01.pdf,2020.0,2.0,1.0,Kaggle Customer Support Twitter,Kaggle Customer Support Twitter,Multi-layered perceptron with binary keyword encoding,41% accuracy for current turn prediction 80% with tolerance,User frustration remains stable across turns limiting dynamic prediction,binary keyword encoding
Bodigutla et al. (2019/2020) Multi-domain Conversation Quality via USE (Response Quality feature sets),,response_clarity,DOWNLOADED,https://assets.amazon.science/20/a7/9fd037fc46d8a892b68adb50cf9c/multi-domain-dialogue-quality-evaluation-via-user-satisfaction-estimation.pdf,2019.0,1.0,0.0,Proprietary multi-domain conversational dataset from Amazon,Proprietary multi-domain conversational dataset from Amazon,"Proposes a multi-task learning approach with a BiLSTM-based model to jointly estimate turn-level and dialogue-level user satisfaction. The model uses features like response quality, user sentiment, and dialogue context.",The proposed multi-task model outperforms single-task baselines in estimating both turn-level and dialogue-level user satisfaction.,"The study uses a proprietary dataset, which is not publicly available, limiting reproducibility. The annotations for user satisfaction are based on a 5-point Likert scale, which can be subjective.","response quality, user sentiment, dialogue context"
Sandbank et al. (2017/2018) Detecting Egregious Conversations... https://aclanthology.org/N18-1163.pdf,,system_failures,DOWNLOADED,https://aclanthology.org/N18-1163.pdf,2018.0,2.0,1.0,Two commercial systems (1100+200 conversations),10-fold cross-validation,SVM classifier with behavioral and interaction features,F1-score 0.59 for egregious detection with 20% improvement over text-only,Small dataset size and limited domain coverage,"behavioral, interaction features"
Zhang et al (2018),"directness_abruptness, hedging_expressions, sarcasm_indicators",,DOWNLOADED,https://arxiv.org/abs/1811.00405,2018.0,19.0,1.0,Wikipedia talk pages,Wikipedia talk pages,Logistic regression with politeness and rhetorical features,Directness and questions predict derailment while hedging predicts civility,Manual annotation required may miss subtle attacks precision over recall,"politeness, rhetorical features"
Oraby et al (2017),"emotion_words, intent_repetition",,DOWNLOADED,https://arxiv.org/abs/1709.05413,2017.0,0.0,2.0,Twitter customer service conversations,Twitter customer service conversations,Develops a fine-grained dialogue act taxonomy for Twitter customer service conversations and uses a sequential SVM-HMM model to predict dialogue acts in real-time. It also analyzes the correlation between dialogue acts and conversation outcomes.,"The type and location of certain dialogue acts have a significant effect on conversation outcomes like customer satisfaction and frustration. The model can predict dialogue acts in real-time, enabling the derivation of actionable rules for automated customer service platforms.","The study is based on data from four companies in three industries, and the findings may not be generalizable to all customer service domains. The dialogue act taxonomy is specific to Twitter and may not be suitable for other communication channels.",dialogue acts
Sandbank et al (2017),,"response_relevance, system_failures",DOWNLOADED,https://arxiv.org/abs/1711.05780,2017.0,2.0,1.0,Two commercial systems (1100+200 conversations),10-fold cross-validation,SVM classifier with behavioral and interaction features,F1-score 0.59 for egregious detection with 20% improvement over text-only,Small dataset size and limited domain coverage,"behavioral, interaction features"
Ultes & Schmitt (2017),conversation_length,,DOWNLOADED,https://www.isca-archive.org/interspeech_2017/ultes17_interspeech.pdf,2017.0,16.0,1.0,LEGO corpus 200 dialogues 4885 turns bus information,Five domains plus real user experiments,Support Vector Machine for interaction quality estimation,Domain-independent estimator achieves similar task success with improved satisfaction,Requires manual quality annotation; degrades with noise,"dialogue act sequences, lexical features, prosodic features, duration-based features"
Chowdhury et al (2016),conversation_length,,DOWNLOADED,https://www.isca-archive.org/interspeech_2016/chowdhury16_interspeech.pdf,2016.0,0.0,1.0,A large dataset of Italian call-center spoken conversations.,A large dataset of Italian call-center spoken conversations.,"Proposes a system for predicting user satisfaction in spoken conversations based on turn-taking features. It uses a turn segmentation and labeling system to extract features like speech overlaps, pauses, and dialogue acts. These features are then used to train a Support Vector Machine (SVM) classifier.","The turn-taking features outperform both prosodic and lexical features in predicting user satisfaction. The analysis of turn-taking features suggests that non-competitive turns and social dialog acts increase the chance of a positive user experience, whereas competitive turns tend to decrease it.","The study is based on a dataset of Italian call-center conversations, which may not be generalizable to other languages or domains. The authors also note that the study is based on a specific set of turn-taking features, and that other features could also be relevant.","turn-taking features (speech overlaps, pauses, dialogue acts)"
Hough & Schlangen (2015),,"intent_repetition, self_corrections, system_misunderstanding_rate, user_corrections",DOWNLOADED,https://aclanthology.org/W15-4647.pdf,2015.0,4.0,0.0,"CamInfo, Let's Go, and SweCC","CamInfo, Let's Go, and SweCC",Presents a data-driven approach for detecting miscommunication in dialogue systems. They use a range of generic features that are both automatically extractable and manually annotated to train two models for online detection and one for offline analysis.,The proposed models performed substantially better than the majority class baseline models.,"The authors note that the study is based on a specific set of features, and that other features could also be relevant. They also note that the study is based on a specific set of dialogue systems, and that the findings may not be generalizable to other systems.","ASR-based features, lexical features, contextual features, prosodic features"
Teh et al (2015),,emphasis_capitalization,DOWNLOADED,https://eprints.lancs.ac.uk/id/eprint/76994/1/iiWAS2015_TEH_PHOEY_LEE_V2_scott.pdf,2015.0,1.0,1.0,1041 product review comments across 10 categories,500 human rater responses on 30 comments,Tested 12 online sentiment analysis tools,Only 20% tools consider exclamation marks; humans show strong correlation,Limited to exclamation marks; most tools ignore punctuation emphasis,exclamation marks
Hutto & Gilbert (2014),,"exclamation_density, negation_frequency",DOWNLOADED,https://ojs.aaai.org/index.php/ICWSM/article/view/14550,2014.0,2.0,0.0,"Social media text (Twitter), movie reviews, product reviews, and news articles.","Social media text (Twitter), movie reviews, product reviews, and news articles.","Introduces VADER (Valence Aware Dictionary and sEntiment Reasoner), a rule-based sentiment analysis tool. VADER uses a human-validated lexicon and considers grammatical and syntactical rules (e.g., punctuation, capitalization, degree modifiers, negation) to determine sentiment intensity.",VADER outperforms individual human raters and several other sentiment analysis benchmarks (including LIWC and machine learning models) in classifying the sentiment of tweets. It generalizes well across different domains.,The study focuses on English-language text and may not be directly applicable to other languages without translation or adaptation of the lexicon and rules. The rules for sentiment intensity are based on a specific set of heuristics and may not capture all nuances of human language.,"lexical, syntactic features (punctuation, capitalization, degree modifiers, negation)"
"— No direct TOD paper modeling delay between obstacle and frustration onset; HCI shows delay-induced frustration effects (West et al., 2011). https://tessawestlab.com/documents/West_2011_The%20effect%20of%20video%20feedback%20delay%20on%20frusteration%20and%20emotion%20communication%20accuracy.pdf",,frustration_delay,DOWNLOADED,https://tessawestlab.com/documents/West_2011_The%20effect%20of%20video%20feedback%20delay%20on%20frusteration%20and%20emotion%20communication%20accuracy.pdf,2011.0,5.0,2.0,Experimental data from 70 participants (35 dyads) in video conversations,Same experimental dataset with delay vs no-delay conditions,Experimental design with video communication system and delay manipulation using Interpersonal Emotions Scale,Delay decreased frustration in period 1 but increased in period 2; improved emotion accuracy in period 1 only,Small sample size; limited to political topics; camera parallax effects; context-dependent delay effects,
Lee & Narayanan (2005),,"confusion_lexical_markers, discourse_markers",DOWNLOADED,https://ict.usc.edu/pubs/Toward%20Detecting%20Emotions%20in%20Spoken%20Dialogs.pdf,2005.0,2.0,0.0,Call center data from a commercially deployed automatic dialog system.,Call center data from a commercially deployed automatic dialog system.,"Proposes a method for detecting negative and non-negative emotions by combining acoustic, lexical, and discourse information. It introduces the concept of 'emotional salience' to identify emotionally charged words and uses discourse markers like rejection and repetition.","Combining acoustic and language information significantly improves emotion classification compared to using only acoustic information. The addition of discourse information does not provide significant further improvement, likely due to its high correlation with the lexical information.","The study is based on a specific, limited-domain call center corpus. The analysis of 'emotional salience' is at the single-word level and does not consider word sequences. The study also assumes perfect speech recognition.","acoustic, lexical, discourse information"
Yildirim et al (2005),,politeness_level,DOWNLOADED,https://www.isca-archive.org/interspeech_2005/yildirim05_interspeech.pdf,2005.0,1.0,1.0,ChIMP database (103 children 15000+ utterances WoZ game),10-fold cross-validation,Linear discriminant classifier with acoustic and language features,84.7% accuracy politeness detection; 71.3% frustration detection with age-dependent models,Limited to children's speech; performance varies significantly by age and gender,"acoustic, language features"
Ceaparu et al (2004),,time_on_task,DOWNLOADED,https://www.cs.umd.edu/~ben/papers/Ceaparu2004Determining.pdf,2004.0,1.0,0.0,N/A (Observational Study),N/A (Observational Study),Used a combination of self-report diaries and observation to collect data on user frustration from 111 participants.,"Frustrating experiences are frequent, with Web browsing, e-mail, and word processing being the most common sources. The most cited causes were error messages, dropped network connections, long download times, and hard-to-find features. A significant amount of time (38-53%) was lost due to frustrating experiences.",The study was conducted with a specific user population (students) and may not be generalizable to all users. The authors also acknowledge that self-reports can be subjective and may not accurately reflect the user's experience.,Not applicable (observational study)
Potamianos et al (2004),,politeness_level,DOWNLOADED,https://slp-ntua.github.io/potam/preprints/conf/05_EURO_kids_emotions.pdf,2004.0,2.0,2.0,"ChIMP database: 103 children (ages 8-14), 15000+ utterances from Carmen Sandiego game",10-fold cross-validation on same dataset,"Linear discriminant classifier combining acoustic features (F0, energy, duration) and language features (word bigram emotional salience)",Age-dependent models achieved 84.7% politeness detection and 71.3% frustration detection accuracy,Language information less effective for frustration due to word choice overlap between neutral and frustrated states,"acoustic features (F0, energy, duration), language features (word bigram emotional salience)"
Arunachalam et al (2001),,"directness_abruptness, politeness_level",DOWNLOADED,https://sail.usc.edu/publications/files/ArunachalamInterSpeech2001.pdf,2001.0,2.0,0.0,ChIMP database,ChIMP database,Wizard of Oz (WOZ) study with 160 children playing a computer game.,"Younger children use fewer politeness markers and express more frustration verbally, especially males.","The study assumed perfect speech recognition and understanding due to the WOZ setup, which doesn't reflect real-world scenarios with recognition errors.","age, sex, task abilities"
Cai & Chen (2020),intent_switch_frequency,,MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
Ghosh et al (2018),,sarcasm_indicators,MANUAL_DOWNLOAD_NEEDED,https://aclanthology.org/J18-4009/,,,,,,,,,Not applicable (manual download needed)
Hertzum (2023),,time_on_task,MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
Higashinaka et al. (2016–2019). Dialogue Breakdown Detection Challenge (DBDC). Surveyed in: Chen et al. (2023) arXiv:2309.06490,,system_misunderstanding_rate,MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
Higashinaka et al (2017),,subgoal_block_count,MANUAL_DOWNLOAD_NEEDED,https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-017-0107-3,,,,,,,,,Not applicable (manual download needed)
Hybrid Service Recovery (2021),escalation_request,,MANUAL_DOWNLOAD_NEEDED,https://scholarspace.manoa.hawaii.edu/bitstreams/43a3e7e5-9c27-4e71-bfc7-53802f41488d/download,,,,,,,,,Not applicable (manual download needed)
Li et al (2019),"intent_repetition, repeated_turns",,MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
Purver (2018),,"confusion_lexical_markers, discourse_markers",MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
RIT Thesis (2009),,response_latency,DOWNLOADED,https://repository.rit.edu/cgi/viewcontent.cgi?article=2374&context=theses,,,,,,,,,Not applicable (experimental study)
Siro et al (2023),,trust_in_system,MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
Steven et al (2023),,alignment_failures,MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
Zhou et al (2022),politeness_level,,MANUAL_DOWNLOAD_NEEDED,NO_LINK_AVAILABLE,,,,,,,,,Not applicable (manual download needed)
