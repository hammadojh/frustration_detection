================================================================================
SYSTEMATIC EARLY FUSION FEATURE SELECTION - COMPLETE ANALYSIS
================================================================================
Experiment completed in 65.7 minutes
Total combinations tested: 41
Target: F1 = 0.8831 (all 22 features)
Reference: F1 = 0.8571 (linguistic 8 features)

COMPLETE RESULTS (sorted by F1 score):
--------------------------------------------------------------------------------

 1. ‚ùå BELOW BASELINE
    Combination: pair_avg_politeness+avg_confusion
    Features: avg_politeness+avg_confusion
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 2. ‚ùå BELOW BASELINE
    Combination: pair_avg_politeness+avg_negation
    Features: avg_politeness+avg_negation
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 3. ‚ùå BELOW BASELINE
    Combination: pair_avg_politeness+avg_turn_length
    Features: avg_politeness+avg_turn_length
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 4. ‚ùå BELOW BASELINE
    Combination: pair_avg_politeness+corrections
    Features: avg_politeness+corrections
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 5. ‚ùå BELOW BASELINE
    Combination: pair_avg_politeness+escalation_requests
    Features: avg_politeness+escalation_requests
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 6. ‚ùå BELOW BASELINE
    Combination: pair_avg_confusion+avg_negation
    Features: avg_confusion+avg_negation
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 7. ‚ùå BELOW BASELINE
    Combination: pair_avg_confusion+avg_turn_length
    Features: avg_confusion+avg_turn_length
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 8. ‚ùå BELOW BASELINE
    Combination: pair_avg_confusion+corrections
    Features: avg_confusion+corrections
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

 9. ‚ùå BELOW BASELINE
    Combination: pair_avg_confusion+escalation_requests
    Features: avg_confusion+escalation_requests
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

10. ‚ùå BELOW BASELINE
    Combination: pair_avg_confusion+avg_urgency
    Features: avg_confusion+avg_urgency
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

11. ‚ùå BELOW BASELINE
    Combination: pair_avg_negation+avg_turn_length
    Features: avg_negation+avg_turn_length
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

12. ‚ùå BELOW BASELINE
    Combination: pair_avg_negation+corrections
    Features: avg_negation+corrections
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

13. ‚ùå BELOW BASELINE
    Combination: pair_avg_negation+escalation_requests
    Features: avg_negation+escalation_requests
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

14. ‚ùå BELOW BASELINE
    Combination: pair_avg_negation+avg_urgency
    Features: avg_negation+avg_urgency
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

15. ‚ùå BELOW BASELINE
    Combination: pair_avg_negation+response_clarity
    Features: avg_negation+response_clarity
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

16. ‚ùå BELOW BASELINE
    Combination: pair_avg_turn_length+corrections
    Features: avg_turn_length+corrections
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

17. ‚ùå BELOW BASELINE
    Combination: pair_avg_turn_length+escalation_requests
    Features: avg_turn_length+escalation_requests
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

18. ‚ùå BELOW BASELINE
    Combination: pair_avg_turn_length+avg_urgency
    Features: avg_turn_length+avg_urgency
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

19. ‚ùå BELOW BASELINE
    Combination: pair_avg_turn_length+response_clarity
    Features: avg_turn_length+response_clarity
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

20. ‚ùå BELOW BASELINE
    Combination: pair_corrections+escalation_requests
    Features: corrections+escalation_requests
    F1 Score: 0.8533
    Feature Count: 2
    vs All Features: -0.0298
    vs Linguistic: -0.0038
    Feature Reduction: 90.9%
    Efficiency: 0.4267 F1/feature

21. ‚ùå BELOW BASELINE
    Combination: top_5_individual
    Features: avg_politeness+avg_confusion+avg_negation+avg_turn_length+corrections
    F1 Score: 0.8462
    Feature Count: 5
    vs All Features: -0.0369
    vs Linguistic: -0.0109
    Feature Reduction: 77.3%
    Efficiency: 0.1692 F1/feature

22. ‚ùå BELOW BASELINE
    Combination: top_6_individual
    Features: avg_politeness+avg_confusion+avg_negation+avg_turn_length+corrections+escalation_requests
    F1 Score: 0.8462
    Feature Count: 6
    vs All Features: -0.0369
    vs Linguistic: -0.0109
    Feature Reduction: 72.7%
    Efficiency: 0.1410 F1/feature

23. ‚ùå BELOW BASELINE
    Combination: linguistic_core
    Features: avg_politeness+avg_confusion+avg_negation+avg_exclamation
    F1 Score: 0.8462
    Feature Count: 4
    vs All Features: -0.0369
    vs Linguistic: -0.0109
    Feature Reduction: 81.8%
    Efficiency: 0.2115 F1/feature

24. ‚ùå BELOW BASELINE
    Combination: mixed_optimal
    Features: avg_politeness+avg_exclamation+task_complexity+response_clarity
    F1 Score: 0.8462
    Feature Count: 4
    vs All Features: -0.0369
    vs Linguistic: -0.0109
    Feature Reduction: 81.8%
    Efficiency: 0.2115 F1/feature

25. ‚ùå BELOW BASELINE
    Combination: top_4_individual
    Features: avg_politeness+avg_confusion+avg_negation+avg_turn_length
    F1 Score: 0.8354
    Feature Count: 4
    vs All Features: -0.0477
    vs Linguistic: -0.0217
    Feature Reduction: 81.8%
    Efficiency: 0.2089 F1/feature

26. ‚ùå BELOW BASELINE
    Combination: triple_avg_politeness+avg_confusion+avg_negation
    Features: avg_politeness+avg_confusion+avg_negation
    F1 Score: 0.8312
    Feature Count: 3
    vs All Features: -0.0519
    vs Linguistic: -0.0259
    Feature Reduction: 86.4%
    Efficiency: 0.2771 F1/feature

27. ‚ùå BELOW BASELINE
    Combination: triple_avg_politeness+avg_confusion+avg_turn_length
    Features: avg_politeness+avg_confusion+avg_turn_length
    F1 Score: 0.8312
    Feature Count: 3
    vs All Features: -0.0519
    vs Linguistic: -0.0259
    Feature Reduction: 86.4%
    Efficiency: 0.2771 F1/feature

28. ‚ùå BELOW BASELINE
    Combination: triple_avg_politeness+avg_confusion+corrections
    Features: avg_politeness+avg_confusion+corrections
    F1 Score: 0.8312
    Feature Count: 3
    vs All Features: -0.0519
    vs Linguistic: -0.0259
    Feature Reduction: 86.4%
    Efficiency: 0.2771 F1/feature

29. ‚ùå BELOW BASELINE
    Combination: triple_avg_politeness+avg_confusion+escalation_requests
    Features: avg_politeness+avg_confusion+escalation_requests
    F1 Score: 0.8312
    Feature Count: 3
    vs All Features: -0.0519
    vs Linguistic: -0.0259
    Feature Reduction: 86.4%
    Efficiency: 0.2771 F1/feature

30. ‚ùå BELOW BASELINE
    Combination: single_avg_politeness
    Features: avg_politeness
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

31. ‚ùå BELOW BASELINE
    Combination: single_avg_confusion
    Features: avg_confusion
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

32. ‚ùå BELOW BASELINE
    Combination: single_avg_negation
    Features: avg_negation
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

33. ‚ùå BELOW BASELINE
    Combination: single_avg_turn_length
    Features: avg_turn_length
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

34. ‚ùå BELOW BASELINE
    Combination: single_corrections
    Features: corrections
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

35. ‚ùå BELOW BASELINE
    Combination: single_escalation_requests
    Features: escalation_requests
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

36. ‚ùå BELOW BASELINE
    Combination: single_avg_urgency
    Features: avg_urgency
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

37. ‚ùå BELOW BASELINE
    Combination: single_response_clarity
    Features: response_clarity
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

38. ‚ùå BELOW BASELINE
    Combination: single_response_relevance
    Features: response_relevance
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

39. ‚ùå BELOW BASELINE
    Combination: single_trust_decline
    Features: trust_decline
    F1 Score: 0.8108
    Feature Count: 1
    vs All Features: -0.0723
    vs Linguistic: -0.0463
    Feature Reduction: 95.5%
    Efficiency: 0.8108 F1/feature

40. ‚ùå BELOW BASELINE
    Combination: single_avg_exclamation
    Features: avg_exclamation
    F1 Score: 0.8000
    Feature Count: 1
    vs All Features: -0.0831
    vs Linguistic: -0.0571
    Feature Reduction: 95.5%
    Efficiency: 0.8000 F1/feature

41. ‚ùå BELOW BASELINE
    Combination: single_task_complexity
    Features: task_complexity
    F1 Score: 0.8000
    Feature Count: 1
    vs All Features: -0.0831
    vs Linguistic: -0.0571
    Feature Reduction: 95.5%
    Efficiency: 0.8000 F1/feature

================================================================================
ANALYSIS BY FEATURE COUNT
================================================================================

1 FEATURES:
  Best: single_avg_politeness (F1=0.8108)
  Features: avg_politeness
  Improvement vs all: -0.0723
  Other combinations tested: 11

2 FEATURES:
  Best: pair_avg_politeness+avg_confusion (F1=0.8533)
  Features: avg_politeness+avg_confusion
  Improvement vs all: -0.0298
  Other combinations tested: 19

3 FEATURES:
  Best: triple_avg_politeness+avg_confusion+avg_negation (F1=0.8312)
  Features: avg_politeness+avg_confusion+avg_negation
  Improvement vs all: -0.0519
  Other combinations tested: 3

4 FEATURES:
  Best: linguistic_core (F1=0.8462)
  Features: avg_politeness+avg_confusion+avg_negation+avg_exclamation
  Improvement vs all: -0.0369
  Other combinations tested: 2

5 FEATURES:
  Best: top_5_individual (F1=0.8462)
  Features: avg_politeness+avg_confusion+avg_negation+avg_turn_length+corrections
  Improvement vs all: -0.0369

6 FEATURES:
  Best: top_6_individual (F1=0.8462)
  Features: avg_politeness+avg_confusion+avg_negation+avg_turn_length+corrections+escalation_requests
  Improvement vs all: -0.0369

================================================================================
FINAL ANSWER
================================================================================
QUESTION: Can we achieve F1‚â•0.88 with fewer features using early fusion?

üìä Best achievable: 2 features ‚Üí F1=0.8533
   Best combination: pair_avg_politeness+avg_confusion
   Features: avg_politeness+avg_confusion

MOST EFFICIENT COMBINATION:
  single_avg_politeness: 0.8108 F1/feature
  Features: avg_politeness
  F1: 0.8108 with 1 features
